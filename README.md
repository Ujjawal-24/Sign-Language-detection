# Sign-Language-Detection-Using-LSTM-and-DTW
# Gesture Recognition with LSTM Networks

## Overview
This project aims to develop a gesture recognition system using Long Short-Term Memory (LSTM) networks to predict sign language actions from videos. The system can handle videos of varying durations and accurately identify the represented gestures, contributing to improved accessibility for hearing-impaired individuals.

## Key Features
- **LSTM Networks:** Utilizes LSTM networks to capture and analyze temporal information, allowing the model to process videos of different lengths effectively.
- **Accurate Prediction:** Achieves a test accuracy of 86% with the help of DTW , indicating the model's ability to accurately predict sign language actions.
- **Real-World Applicability:** Suitable for real-world applications, enhancing communication between hearing-impaired individuals and the broader community.
- **Potential for Integration:** Can be integrated into assistive technologies and devices for real-time translation of sign language gestures.

## Usage
1. **Data Collection and Preprocessing:**
   - Collect videos of sign language actions and organize them into datasets.
   - Preprocess the videos to extract key features and prepare the data for training.

2. **Training the Model:**
   - Use the provided Python script to train the gesture recognition model.
   - Adjust parameters and network architecture as needed.
   - Train the model using the training dataset.

3. **Evaluation:**
   - Evaluate the trained model using the test dataset to measure its accuracy.
   - Analyze the performance and fine-tune the model if necessary.

Link for data used in training the model: https://drive.google.com/drive/folders/1fWrmm6H5gG2G8yBpjvGncPr_uAaFfkmp
